{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bridging Perception and Reasoning to Build Interpretable Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge in Perception\n",
    "\n",
    "I have previosly explored the way in which knowledge, understanding and explanation relate to each other. In this research project I'm curious as to how different artificial intelligence approaches relate to human cognitive processes. \n",
    "\n",
    "There is much confounding of intelligence and knowledge in the social science literature, however, [GREGORY1997KNOWLEDGE] attempts to separate the concepts: \n",
    "    ''more knowledge can reduce the need intelligence needed for solving problems''.\n",
    "    \n",
    "To me this is really interesting becuase it relates back to the argument against explainable AI presented by Cynthia Rudin: that we should focus our energy on extracting better knowledge from our data (or prior experience) instead of attempting to explain deep models. This is something I'm thinking about in my other research projects LINK HERE.\n",
    "\n",
    "This research project focuses instead on perception and reasoning, how important these two tasks for human intelligence and how distinct they are. I want to apply these ideas to develop more human like artifical intelligence that is hopefully more interpetable. \n",
    "\n",
    "[GREGORY1997KNOWLEDGE] separates intelligence into two components:\n",
    "<ul>\n",
    "    <li> Kinetic intelligence: active processing of information </li>\n",
    "    <li> Potential intelligence: Stored from the past potential intelligence of knowledge that is selected and applied to current situation. </li> \n",
    "    </ul>\n",
    "  \n",
    "[GREGORY1997KNOWLEDGE] Knowledge is necesarry for vision because sensory information is inherently ambiguous, simialarly, errors of perception can be attributed to knowledge being misapplied. Some way of creating artifical potential intelligence is therefore necessary for AI to be considered more 'brain-like'. \n",
    " \n",
    "If reasoning is the human brain's capacity of making sense of things based on new or existing infromation then we can equate it to the way the brain balances kinetic and potential intelligence. This process can be explored a little better through considering the behaviour of the brain when presented with an optical illusion.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optical Illusions\n",
    "\n",
    "\n",
    "Optical illusions are images that confuse the brain. The types of optical illusions include:\n",
    "<ul> \n",
    "    <li> Literal optical illusion - when the image you see is different from the images that make it up </li>\n",
    "    <li> Physiological optical illusion: overstimulation of the brain's senses </li>\n",
    "<li> Cognitive optical illusions: how the subconscious mind thinks and how it relates one object to another </li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "[GREGORY1997KNOWLEDGE] argues that <em> \"it is significant that illusions are experienced perctually despite the observer knowing conceptually that these are illusory - even to the point of knowing the causes of the phenomena.\" </em> This seems to imply that conceptual and perceptual knowledge are independent and this is reflected in the psychological literature.\n",
    "    \n",
    "## Optical Illusions in Health\n",
    "\n",
    "put https://pubmed.ncbi.nlm.nih.gov/24224600/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Intelligence in AI\n",
    "\n",
    "We all know that machine learning is very good at mapping sensory content to a concept but we also all know that machine learning is NOT very good at learning simple relations or putting things into context. Why is this the case?\n",
    "[DAI2019BRIDGING] argues that ML is just curve fitting and has no explicit model of reasoning. I want to take this a bit further and explore how machine learning fits into kinetic or potential intelligence and how it fits into reasoning and perception.\n",
    "\n",
    "## Explainability's Dependence on Perception\n",
    "\n",
    "Treating reason as perception is v bad - (panda noise, Goodfellow et al 2015) possibly a cause of non-robustness of explanations\n",
    "Treating reason as perception also brings another problem \n",
    "\t: 1 hard to tell machines what we know\n",
    "\t: 2 hard to understand how machines learn (interpretable AI)\n",
    "The real gap is here\n",
    "\n",
    "explainability uses sensory input to reason about the models behaviour\n",
    "\n",
    "why explain with reference to feature importance\n",
    "\n",
    "what we should be doing is bridging the gap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Can Reasoning Bring\n",
    "\n",
    "what is reasoning in artifical intelligence?\n",
    "\n",
    "Machine reasoning was the first task in AI  was then developed as symbolic AI\n",
    "Inductive logic programming, probabilistic logic programming, constraint logic programming: “a physical symbol systems has the necessary and sufficient means for general intelligent action\n",
    "\n",
    "why has symbolic AI become so unattractive?\n",
    "needs to be hard programmed\n",
    "stuart russell \"symbols dont exist\"\n",
    "intereting! links to representation learning\n",
    "\n",
    "## Perception and Reasoning in Biomedical Data\n",
    "\n",
    "Deep machine learning has tendency to overfit - seek more training data - this doesnt work in health data\n",
    "\n",
    "extremely data efficient, but cant be applied to ambiguous data types like images \n",
    "\n",
    "## Hybrid\n",
    "\n",
    "connectionist networks program indutcion literature review?\n",
    "\n",
    "open challenge how to combine the continuous perception module with the combinatroial/discrete reasoning module\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State-of-the-art\n",
    "\n",
    "### [DAI2019BRIDGING]\n",
    "\n",
    "### [EVANS2018LEARNING]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link to explainability of biomedical data and what i want to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "<ul> \n",
    "    <li> [GREGORY1997KNOWLEDGE] Gregory, Richard L. \"Knowledge in perception and illusion.\" Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences 352.1358 (1997): 1121-1127. </li>\n",
    "      <li> [KOMPRIDIS2000REASON] Kompridis, Nikolas (2000). \"So We Need Something Else for Reason to Mean\". International Journal of Philosophical Studies. 8 (3): 271–295. doi:10.1080/096725500750039282. </li>\n",
    "    <li> https://www.cleareyes.com/eye-care-blog/201610/types-optical-illusions/#:~:text=There%20are%20three%20main%20types,%E2%80%9Ctrick%E2%80%9D%20of%20the%20eye. </li>\n",
    "    <li> [DAI2019BRIDGING] Dai, Wang-Zhou, et al. \"Bridging machine learning and logical reasoning by abductive learning.\" Advances in Neural Information Processing Systems. 2019. </li>\n",
    "    <li> [EVANS2018LEARNING] Evans, Richard, and Edward Grefenstette. \"Learning explanatory rules from noisy data.\" Journal of Artificial Intelligence Research 61 (2018): 1-64. </li> \n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
